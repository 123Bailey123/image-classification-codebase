max_epochs: 90

log_interval: 10

use_amp: true # if true, it will train in automatic mixed precision mode

only_evaluate: false # if true, it will only evalute the model on the validation set and exit

auto_resume: true # if true, it will automatically load the checkpoint in the output directory and continue to train

sync_batchnorm: false # if true, it will convert all the batchnorm layers into torch.nn.SyncBatchNorm

accmulated_steps: 1

set_reproducible: false # if true, the training will be set to reproducible (refer to https://pytorch.org/docs/stable/notes/randomness.html)
                        # else torch.backends.cudnn.benchmark will be set to True for largest throughput

data {
    type_: imagenet2012

    image_size: 224
    num_classes: 1000

    root: data/imagenet2012-wds

    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

    batch_size: 128
    num_workers: 8

    use_dali: true # if ture, use NVIDIA DALI to preprocess the images instead of torchvision
    use_webdataset: true # if ture, use webdataset to load images instead of loading from image folder

    trainset_len: 1281167 # required when use_webdataset=true
    valset_len: 50000 # required when use_webdataset=true
}

model {
    type_: resnet50
    pretrained: false
    load_from: null
}

optimizer {
    type_: CustomSGD
    lr: 0.256
    basic_bs: 256 # we set lr=0.256 for 256 batch size, for other batch sizes we linearly scale the learning rate.
    momentum: 0.875
    dampening: 0
    weight_decay: 3.0517578125e-05 # refer to https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5
    nesterov: false
    bn_weight_decay: 0
}

scheduler {
    type_: WarmupCosineAnnealingLR
    T_warmup: 5
    T_max: ${max_epochs}
    eta_min: 0
}

criterion {
    type_: LabelSmoothCrossEntropyLoss
    num_classes: ${data.num_classes}
    epsilon: 0.1
}
